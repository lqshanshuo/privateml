{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pond.tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-43fe22466f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNativeTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrivateEncodedTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPublicEncodedTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReveal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pond.tensor"
     ]
    }
   ],
   "source": [
    "from pond.tensor import NativeTensor, PrivateEncodedTensor, PublicEncodedTensor\n",
    "from pond.nn import Dense, Sigmoid, Reveal, Diff, Softmax, CrossEntropy, Sequential, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = dataset\n",
    "    \n",
    "    # NOTE: this is the shape used by Tensorflow; other backends may differ\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test  = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    \n",
    "    x_train  = x_train.astype('float32')\n",
    "    x_test   = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test  /= 255\n",
    "\n",
    "    y_train = to_categorical(y_train, 5)\n",
    "    y_test  = to_categorical(y_test, 5)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    x_train_public = x_train[y_train < 5]\n",
    "    y_train_public = y_train[y_train < 5]\n",
    "    x_test_public  = x_test[y_test < 5]\n",
    "    y_test_public  = y_test[y_test < 5]\n",
    "    public_dataset = (x_train_public, y_train_public), (x_test_public, y_test_public)\n",
    "\n",
    "    x_train_private = x_train[y_train >= 5]\n",
    "    y_train_private = y_train[y_train >= 5] - 5\n",
    "    x_test_private  = x_test[y_test >= 5]\n",
    "    y_test_private  = y_test[y_test >= 5] - 5\n",
    "    private_dataset = (x_train_private, y_train_private), (x_test_private, y_test_private)\n",
    "    \n",
    "    return preprocess_data(public_dataset), preprocess_data(private_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train on public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/1\n",
      "30596/30596 [==============================] - 70s 2ms/step - loss: 0.5809 - acc: 0.7816 - val_loss: 0.1198 - val_acc: 0.9661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2e4ead50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_dataset, private_dataset = load_data()\n",
    "\n",
    "feature_layers = [\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),\n",
    "    keras.layers.Activation('sigmoid'),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same'),\n",
    "    keras.layers.Activation('sigmoid'),\n",
    "    keras.layers.AveragePooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Dropout(.25),\n",
    "    keras.layers.Flatten()\n",
    "]\n",
    "\n",
    "classification_layers = [\n",
    "    keras.layers.Dense(128),\n",
    "    keras.layers.Activation('sigmoid'),\n",
    "    keras.layers.Dropout(.50),\n",
    "    keras.layers.Dense(5),\n",
    "    keras.layers.Activation('softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(feature_layers + classification_layers)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = public_dataset\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from private data (unencrypted for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 813,157\n",
      "Trainable params: 813,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = model.get_layer(index=6)\n",
    "assert flatten_layer.name.startswith('flatten_')\n",
    "\n",
    "extractor = keras.models.Model(\n",
    "    inputs=model.input, \n",
    "    outputs=flatten_layer.output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_images, y_train), (x_test_images, y_test) = private_dataset\n",
    "\n",
    "x_train_features = extractor.predict(x_train_images)\n",
    "x_test_features  = extractor.predict(x_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('x_train_features.npy', x_train_features)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "np.save('x_test_features.npy', x_test_features)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29404, 6272) (29404, 5) (4861, 6272) (4861, 5)\n"
     ]
    }
   ],
   "source": [
    "x_train_features = np.load('x_train_features.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "x_test_features = np.load('x_test_features.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "print(x_train_features.shape, y_train.shape, x_test_features.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential([\n",
    "    Dense(128, 6272),\n",
    "    Sigmoid(),\n",
    "    # Dropout(.5),\n",
    "    Dense(5, 128),\n",
    "    Reveal(),\n",
    "    Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(classifier, x, y, verbose=0, wrapper=NativeTensor):\n",
    "    predicted_classes = classifier \\\n",
    "        .predict(DataLoader(x, wrapper), verbose=verbose).reveal() \\\n",
    "        .argmax(axis=1)\n",
    "        \n",
    "    correct_classes = NativeTensor(y) \\\n",
    "        .argmax(axis=1)\n",
    "        \n",
    "    matches = predicted_classes.unwrap() == correct_classes.unwrap()\n",
    "    return sum(matches)/len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29404, 6272)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... using NativeTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:02:50.827373 Epoch 0\n",
      "2017-12-29 11:03:07.306861 Epoch 1\n",
      "2017-12-29 11:03:22.950060 Epoch 2\n",
      "Elapsed: 0:00:48.221433\n"
     ]
    }
   ],
   "source": [
    "classifier.initialize()\n",
    "\n",
    "start = datetime.now()\n",
    "classifier.fit(\n",
    "    DataLoader(x_train_features, wrapper=NativeTensor), \n",
    "    DataLoader(y_train, wrapper=NativeTensor), \n",
    "    loss=CrossEntropy(), \n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "stop = datetime.now()\n",
    "\n",
    "print(\"Elapsed:\", stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.90671337233\n",
      "Test accuracy: 0.908660769389\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", accuracy(classifier, x_train_features, y_train))\n",
    "print(\"Test accuracy:\",  accuracy(classifier, x_test_features,  y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... using PublicEncodedTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.initialize()\n",
    "\n",
    "start = datetime.now()\n",
    "classifier.fit(\n",
    "    DataLoader(x_train_features, wrapper=PublicEncodedTensor), \n",
    "    DataLoader(y_train, wrapper=PublicEncodedTensor),\n",
    "    loss=CrossEntropy(), \n",
    "    epochs=3,\n",
    "    verbose=2\n",
    ")\n",
    "stop = datetime.now()\n",
    "\n",
    "print(\"Elapsed:\", stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", accuracy(classifier, x_train_features, y_train, verbose=2))\n",
    "print(\"Test accuracy:\",  accuracy(classifier, x_test_features,  y_test,  verbose=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... using PrivateEncodedTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:03:42.926628 Epoch 0\n",
      "2017-12-29 11:03:43.243759 Batch 0\n",
      "2017-12-29 11:04:54.253005 Batch 1\n",
      "2017-12-29 11:06:37.272832 Batch 2\n",
      "2017-12-29 11:08:20.516759 Batch 3\n",
      "2017-12-29 11:09:56.260065 Batch 4\n",
      "2017-12-29 11:11:30.663451 Batch 5\n",
      "2017-12-29 11:13:04.473740 Batch 6\n",
      "2017-12-29 11:14:39.077599 Batch 7\n"
     ]
    }
   ],
   "source": [
    "classifier.initialize()\n",
    "\n",
    "start = datetime.now()\n",
    "classifier.fit(\n",
    "    DataLoader(x_train_features, wrapper=PrivateEncodedTensor), \n",
    "    DataLoader(y_train, wrapper=PrivateEncodedTensor),\n",
    "    loss=CrossEntropy(), \n",
    "    epochs=3,\n",
    "    verbose=2\n",
    ")\n",
    "stop = datetime.now()\n",
    "\n",
    "print(\"Elapsed:\", stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-02 14:13:39.032988 Batch 0\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = accuracy(classifier, x_train_features, y_train, verbose=2)\n",
    "test_accuracy  = accuracy(classifier, x_test_features,  y_test,  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.906611345395\n",
      "Test accuracy: 0.908249331413\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", train_accuracy)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('layer0_weights_0.npy', classifier.layers[0].weights.shares0)\n",
    "np.save('layer0_weights_1.npy', classifier.layers[0].weights.shares1)\n",
    "np.save('layer0_bias_0.npy', classifier.layers[0].bias.shares0)\n",
    "np.save('layer0_bias_1.npy', classifier.layers[0].bias.shares1)\n",
    "\n",
    "np.save('layer2_weights_0.npy', classifier.layers[2].weights.shares0)\n",
    "np.save('layer2_weights_1.npy', classifier.layers[2].weights.shares1)\n",
    "np.save('layer2_bias_0.npy', classifier.layers[2].bias.shares0)\n",
    "np.save('layer2_bias_1.npy', classifier.layers[2].bias.shares1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow27]",
   "language": "python",
   "name": "conda-env-tensorflow27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
